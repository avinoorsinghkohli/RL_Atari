{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca270cff",
   "metadata": {
    "id": "9Yar48yqGpfA",
    "papermill": {
     "duration": 0.007723,
     "end_time": "2024-05-04T08:15:52.930524",
     "exception": false,
     "start_time": "2024-05-04T08:15:52.922801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Intialising the notebook and folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c5fca",
   "metadata": {
    "id": "XC64UU4f5e_u",
    "papermill": {
     "duration": 0.006731,
     "end_time": "2024-05-04T08:15:52.958067",
     "exception": false,
     "start_time": "2024-05-04T08:15:52.951336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de475a89",
   "metadata": {
    "id": "PK0XsHWr5e_x",
    "papermill": {
     "duration": 0.006749,
     "end_time": "2024-05-04T08:15:52.971810",
     "exception": false,
     "start_time": "2024-05-04T08:15:52.965061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df5ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:15:52.987075Z",
     "iopub.status.busy": "2024-05-04T08:15:52.986753Z",
     "iopub.status.idle": "2024-05-04T08:16:18.680800Z",
     "shell.execute_reply": "2024-05-04T08:16:18.679826Z"
    },
    "id": "fYZDCZ7t5e_y",
    "outputId": "0333b4b9-ce84-4107-fd54-6a2387f1bea3",
    "papermill": {
     "duration": 25.704773,
     "end_time": "2024-05-04T08:16:18.683527",
     "exception": false,
     "start_time": "2024-05-04T08:15:52.978754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install gym==0.26.2 gym-notices==0.0.8\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b8038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:16:18.709848Z",
     "iopub.status.busy": "2024-05-04T08:16:18.709157Z",
     "iopub.status.idle": "2024-05-04T08:17:27.234987Z",
     "shell.execute_reply": "2024-05-04T08:17:27.233781Z"
    },
    "id": "wB39Yv6W5e_z",
    "outputId": "8c6eda31-b02f-48b4-ffc4-daaa47798bbf",
    "papermill": {
     "duration": 68.541521,
     "end_time": "2024-05-04T08:17:27.237729",
     "exception": false,
     "start_time": "2024-05-04T08:16:18.696208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup\n",
    "!pip3 install gym[atari]\n",
    "!pip3 install gym[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefb5532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:27.273359Z",
     "iopub.status.busy": "2024-05-04T08:17:27.273041Z",
     "iopub.status.idle": "2024-05-04T08:17:27.278881Z",
     "shell.execute_reply": "2024-05-04T08:17:27.278146Z"
    },
    "id": "F0BwQTHAQCn8",
    "papermill": {
     "duration": 0.025163,
     "end_time": "2024-05-04T08:17:27.280703",
     "exception": false,
     "start_time": "2024-05-04T08:17:27.255540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for DQN agent, memory and training\n",
    "EPISODES = 3500\n",
    "HEIGHT = 84\n",
    "WIDTH = 84\n",
    "HISTORY_SIZE = 4\n",
    "learning_rate = 0.0001\n",
    "evaluation_reward_length = 100\n",
    "Memory_capacity = 1000000\n",
    "train_frame = 100000 # You can set it to a lower value while testing your code so you don't have to wait longer to see if the training code does not have any syntax errors\n",
    "batch_size = 128\n",
    "scheduler_gamma = 0.4\n",
    "scheduler_step_size = 100000\n",
    "\n",
    "# Hyperparameters for Double DQN agent\n",
    "update_target_network_frequency = 1000\n",
    "\n",
    "# Hyperparameters for DQN LSTM agent\n",
    "lstm_seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72764b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:27.314553Z",
     "iopub.status.busy": "2024-05-04T08:17:27.314304Z",
     "iopub.status.idle": "2024-05-04T08:17:31.500201Z",
     "shell.execute_reply": "2024-05-04T08:17:31.499415Z"
    },
    "id": "bdbuEMKxe_55",
    "papermill": {
     "duration": 4.205242,
     "end_time": "2024-05-04T08:17:31.502520",
     "exception": false,
     "start_time": "2024-05-04T08:17:27.297278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27259540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.538232Z",
     "iopub.status.busy": "2024-05-04T08:17:31.537444Z",
     "iopub.status.idle": "2024-05-04T08:17:31.544050Z",
     "shell.execute_reply": "2024-05-04T08:17:31.543213Z"
    },
    "id": "oyaON2t0P38g",
    "papermill": {
     "duration": 0.026176,
     "end_time": "2024-05-04T08:17:31.545895",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.519719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_max_lives(env):\n",
    "    env.reset()\n",
    "    _, _, _, _, info = env.step(0)\n",
    "    return info['lives']\n",
    "\n",
    "def check_live(life, cur_life):\n",
    "    if life > cur_life:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_frame(X):\n",
    "    x = np.uint8(resize(rgb2gray(X), (HEIGHT, WIDTH), mode='reflect') * 255)\n",
    "    return x\n",
    "\n",
    "def get_init_state(history, s, history_size):\n",
    "    for i in range(history_size):\n",
    "        history[i, :, :] = get_frame(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b3dd6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.579876Z",
     "iopub.status.busy": "2024-05-04T08:17:31.579604Z",
     "iopub.status.idle": "2024-05-04T08:17:31.593298Z",
     "shell.execute_reply": "2024-05-04T08:17:31.592533Z"
    },
    "id": "DyxnRXNYQIeB",
    "papermill": {
     "duration": 0.03309,
     "end_time": "2024-05-04T08:17:31.595198",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.562108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(3136, 512)\n",
    "        self.head = nn.Linear(512, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class DQN_LSTM(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(DQN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(3136, 512)\n",
    "        self.head = nn.Linear(256, action_size)\n",
    "        # Define an LSTM layer\n",
    "\n",
    "    def forward(self, x, hidden = None, train=True):\n",
    "        if train==True: # If training, we will merge all the visual states into one. So first two dimensions (batch_size, lstm_seq_length) will be merged into one. This will let us process all these together.\n",
    "            x = x.view(-1, 1, HEIGHT, WIDTH)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
    "        if train==True: # We will reshape the output to match the original shape. So first dimension will be extended back to (batch_size, lstm_seq_length)\n",
    "            x = x.view(-1, lstm_seq_length, 512)\n",
    "        # Pass the state through an LSTM\n",
    "        ### CODE ###\n",
    "\n",
    "        return self.head(lstm_output), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56a7dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.629103Z",
     "iopub.status.busy": "2024-05-04T08:17:31.628863Z",
     "iopub.status.idle": "2024-05-04T08:17:31.640987Z",
     "shell.execute_reply": "2024-05-04T08:17:31.640336Z"
    },
    "id": "1C6gBElrQgTp",
    "papermill": {
     "duration": 0.031301,
     "end_time": "2024-05-04T08:17:31.642821",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.611520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=Memory_capacity)\n",
    "\n",
    "    def push(self, history, action, reward, done):\n",
    "        self.memory.append((history, action, reward, done))\n",
    "\n",
    "    def sample_mini_batch(self, frame):\n",
    "        mini_batch = []\n",
    "        if frame >= Memory_capacity:\n",
    "            sample_range = Memory_capacity\n",
    "        else:\n",
    "            sample_range = frame\n",
    "\n",
    "        # history size\n",
    "        sample_range -= (HISTORY_SIZE + 1)\n",
    "\n",
    "        idx_sample = random.sample(range(sample_range), batch_size)\n",
    "        for i in idx_sample:\n",
    "            sample = []\n",
    "            for j in range(HISTORY_SIZE + 1):\n",
    "                sample.append(self.memory[i + j])\n",
    "\n",
    "            sample = np.array(sample, dtype=object)\n",
    "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[3, 1], sample[3, 2], sample[3, 3]))\n",
    "\n",
    "        return mini_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class ReplayMemoryLSTM(ReplayMemory):\n",
    "    \"\"\"\n",
    "    This is a version of Replay Memory modified for LSTMs.\n",
    "    Replay memory generally stores (state, action, reward, next state).\n",
    "    But LSTMs need sequential data.\n",
    "    So we store (state, action, reward, next state) for previous few states, constituting a trajectory.\n",
    "    During training, the previous states will be used to generate the current state of LSTM.\n",
    "    Note that samples from previous episode might get included in the trajectory.\n",
    "    Inspite of not being fully correct, this simple Replay Buffer performs well.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def sample_mini_batch(self, frame):\n",
    "        mini_batch = []\n",
    "        if frame >= Memory_capacity:\n",
    "            sample_range = Memory_capacity\n",
    "        else:\n",
    "            sample_range = frame\n",
    "\n",
    "        sample_range -= (lstm_seq_length + 1)\n",
    "\n",
    "        idx_sample = random.sample(range(sample_range - lstm_seq_length), batch_size)\n",
    "        for i in idx_sample:\n",
    "            sample = []\n",
    "            for j in range(lstm_seq_length + 1):\n",
    "                sample.append(self.memory[i + j])\n",
    "\n",
    "            sample = np.array(sample, dtype=object)\n",
    "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[lstm_seq_length - 1, 1], sample[lstm_seq_length - 1, 2], sample[lstm_seq_length - 1, 3]))\n",
    "\n",
    "        return mini_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ef329a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.677480Z",
     "iopub.status.busy": "2024-05-04T08:17:31.677208Z",
     "iopub.status.idle": "2024-05-04T08:17:31.683591Z",
     "shell.execute_reply": "2024-05-04T08:17:31.682743Z"
    },
    "id": "PVMQP8znPvAx",
    "papermill": {
     "duration": 0.026021,
     "end_time": "2024-05-04T08:17:31.685567",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.659546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Agent():\n",
    "#     def __init__(self, action_size):\n",
    "#         self.action_size = action_size\n",
    "\n",
    "#         # These are hyper parameters for the DQN\n",
    "#         self.discount_factor = 0.99\n",
    "#         self.epsilon = 1.0\n",
    "#         self.epsilon_min = 0.01\n",
    "#         self.explore_step = 500000\n",
    "#         self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
    "#         self.train_start = 100000\n",
    "#         self.update_target = 1000\n",
    "\n",
    "#         # Generate the memory\n",
    "#         self.memory = ReplayMemory()\n",
    "\n",
    "#         # Create the policy net\n",
    "#         self.policy_net = DQN(action_size)\n",
    "#         self.policy_net.to(device)\n",
    "\n",
    "#         self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
    "#         self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "#     def load_policy_net(self, path):\n",
    "#         self.policy_net = torch.load(path)\n",
    "\n",
    "#     \"\"\"Get action using policy net using epsilon-greedy policy\"\"\"\n",
    "#     def get_action(self, state):\n",
    "#         if np.random.rand() <= self.epsilon:\n",
    "#             ### CODE ####\n",
    "#             action = torch.tensor(random.randrange(self.action_size)).to(device)\n",
    "#         else:\n",
    "#             ### CODE ####\n",
    "#             state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "#             action = self.policy_net(state).max(1)[1].unsqueeze(0)\n",
    "#         return action\n",
    "\n",
    "#     # pick samples randomly from replay memory (with batch_size)\n",
    "#     def train_policy_net(self, frame):\n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon -= self.epsilon_decay\n",
    "\n",
    "#         mini_batch = self.memory.sample_mini_batch(frame)\n",
    "#         mini_batch = np.array(mini_batch, dtype=object).transpose()\n",
    "\n",
    "#         history = np.stack(mini_batch[0], axis=0)\n",
    "#         states = np.float32(history[:, :4, :, :]) / 255.\n",
    "#         states = torch.from_numpy(states).cuda()\n",
    "#         actions = list(mini_batch[1])\n",
    "#         actions = torch.LongTensor(actions).cuda()\n",
    "#         rewards = list(mini_batch[2])\n",
    "#         rewards = torch.FloatTensor(rewards).cuda()\n",
    "#         next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
    "#         next_states = torch.from_numpy(next_states).to(device)\n",
    "#         dones = mini_batch[3] # checks if the game is over\n",
    "#         mask = torch.tensor(list(map(int, dones==False)),dtype=torch.uint8).to(device)\n",
    "\n",
    "#         self.optimizer.zero_grad()\n",
    "#         # Compute Q(s_t, a), the Q-value of the current state\n",
    "#         ### CODE ####\n",
    "#         state_action_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "#         # Compute Q function of next state\n",
    "#         ### CODE ####\n",
    "#         next_state_actions = self.policy_net(next_states)\n",
    "#         # Find maximum Q-value of action at next state from policy net\n",
    "#         ### CODE ####\n",
    "#         next_state_val = next_state_actions.max(1)[0] * mask.float()\n",
    "#         expected_next_state_values = (next_state_val.detach() * self.discount_factor) + rewards\n",
    "#         # Compute the Huber Loss\n",
    "#         ### CODE ####\n",
    "#         criterion = nn.SmoothL1Loss()\n",
    "#         loss = criterion(state_action_values, expected_next_state_values)\n",
    "\n",
    "#         # Optimize the model, .step() both the optimizer and the scheduler!\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         self.scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517e7672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.721900Z",
     "iopub.status.busy": "2024-05-04T08:17:31.721628Z",
     "iopub.status.idle": "2024-05-04T08:17:31.786213Z",
     "shell.execute_reply": "2024-05-04T08:17:31.785417Z"
    },
    "id": "6gmTWpwkQ5D4",
    "papermill": {
     "duration": 0.086061,
     "end_time": "2024-05-04T08:17:31.788140",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.702079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.explore_step = 500000\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
    "        self.train_start = 100000\n",
    "        self.update_target = 1000\n",
    "\n",
    "        # Generate the memory\n",
    "        self.memory = ReplayMemory()\n",
    "\n",
    "        # Create the policy net and the target net\n",
    "        self.policy_net = DQN(action_size)\n",
    "        self.policy_net.to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "        # Initialize a target network and initialize the target network to the policy net\n",
    "        self.target_net = DQN(action_size).to(device)\n",
    "        self.target_net.eval()\n",
    "\n",
    "    def load_policy_net(self, path):\n",
    "        self.policy_net = torch.load(path)           \n",
    "\n",
    "    # after some time interval update the target net to be same with policy net\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "\n",
    "    \"\"\"Get action using policy net using epsilon-greedy policy\"\"\"\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = torch.tensor(random.randrange(self.action_size)).to(device)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action = self.policy_net(state).max(1)[1].unsqueeze(0)\n",
    "        return action\n",
    "\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_policy_net(self, frame):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        mini_batch = self.memory.sample_mini_batch(frame)\n",
    "        mini_batch = np.array(mini_batch, dtype=object).transpose()\n",
    "\n",
    "        history = np.stack(mini_batch[0], axis=0)\n",
    "        states = np.float32(history[:, :4, :, :]) / 255.\n",
    "        states = torch.from_numpy(states).cuda()\n",
    "        actions = list(mini_batch[1])\n",
    "        actions = torch.LongTensor(actions).cuda()\n",
    "        rewards = list(mini_batch[2])\n",
    "        rewards = torch.FloatTensor(rewards).cuda()\n",
    "        next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
    "        next_states = torch.from_numpy(next_states).to(device)\n",
    "        dones = mini_batch[3] # checks if the game is over\n",
    "        mask = torch.tensor(list(map(int, dones==False)),dtype=torch.uint8).cuda()\n",
    "        \n",
    "        # Your agent.py code here with double DQN modifications\n",
    "        ### CODE ###\n",
    "        curr_state_actions = self.policy_net(states)\n",
    "        curr_state_values = curr_state_actions.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_curr_actions = self.policy_net(next_states)\n",
    "        next_curr_actions = next_curr_actions.max(1)[1]\n",
    "        # print(\"hi\")\n",
    "        # next_curr_actions = next_curr_actions.detach()\n",
    "        # Compute Q function of next state using the same policy network\n",
    "        next_state_actions = self.target_net(next_states).detach()\n",
    "        next_state_values = next_state_actions.gather(1, next_curr_actions.unsqueeze(1)).squeeze(1)\n",
    "        next_state_values = next_state_values * mask.float()\n",
    "\n",
    "        # Compute the expected Q values using the Bellman equation\n",
    "        expected_state_values = rewards + self.discount_factor * next_state_values\n",
    "\n",
    "        # Compute the Huber Loss between current and expected Q values\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(curr_state_values, expected_state_values)\n",
    "\n",
    "        # Optimize the model: perform backpropagation and update weights\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454cf3f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.822514Z",
     "iopub.status.busy": "2024-05-04T08:17:31.822256Z",
     "iopub.status.idle": "2024-05-04T08:17:31.833690Z",
     "shell.execute_reply": "2024-05-04T08:17:31.832912Z"
    },
    "id": "q9hYjI-IQxWE",
    "papermill": {
     "duration": 0.030516,
     "end_time": "2024-05-04T08:17:31.835503",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.804987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent_LSTM(Agent):\n",
    "    def __init__(self, action_size):\n",
    "        super().__init__(action_size)\n",
    "        # Generate the memory\n",
    "        self.memory = ReplayMemoryLSTM()\n",
    "\n",
    "        # Create the policy net\n",
    "        self.policy_net = DQN_LSTM(action_size)\n",
    "        self.policy_net.to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "\n",
    "    \"\"\"Get action using policy net using epsilon-greedy policy\"\"\"\n",
    "    def get_action(self, state, hidden=None):\n",
    "        ### CODE ###\n",
    "        # Similar to that for Agent\n",
    "        # You should pass the state and hidden through the policy net even when you are randomly selecting an action so you can get the hidden state for the next state\n",
    "        # We recommend the following outline:\n",
    "        # 1. Pass the state and hidden through the policy net. You should pass train=False to the forward function of the policy net here becasue you are not training the policy net here\n",
    "        # 2. If you are randomly selecting an action, return the random action and policy net's hidden, otherwise return the policy net's action and hidden\n",
    "        return a, hidden\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_policy_net(self, frame):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        mini_batch = self.memory.sample_mini_batch(frame)\n",
    "        mini_batch = np.array(mini_batch, dtype=object).transpose()\n",
    "\n",
    "        history = np.stack(mini_batch[0], axis=0)\n",
    "        states = np.float32(history[:, :lstm_seq_length, :, :]) / 255.\n",
    "        states = torch.from_numpy(states).cuda()\n",
    "        actions = list(mini_batch[1])\n",
    "        actions = torch.LongTensor(actions).cuda()\n",
    "        rewards = list(mini_batch[2])\n",
    "        rewards = torch.FloatTensor(rewards).cuda()\n",
    "        next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
    "        dones = mini_batch[3] # checks if the game is over\n",
    "        mask = torch.tensor(list(map(int, dones==False)),dtype=torch.uint8)\n",
    "\n",
    "        ### All the following code is nearly same as that for Agent\n",
    "\n",
    "        # Compute Q(s_t, a), the Q-value of the current state\n",
    "        # You should hidden=None as input to policy_net. It will return lstm_state and hidden. Discard hidden. Use the last lstm_state as the current Q values\n",
    "        ### CODE ####\n",
    "\n",
    "        # Compute Q function of next state\n",
    "        # Similar to previous, use hidden=None as input to policy_net. And discard the hidden returned by policy_net\n",
    "        ### CODE ####\n",
    "\n",
    "        # Find maximum Q-value of action at next state from policy net\n",
    "        # Use the last lstm_state as the Q values of next state\n",
    "        ### CODE ####\n",
    "\n",
    "        # Compute the Huber Loss\n",
    "        ### CODE ####\n",
    "\n",
    "        # Optimize the model, .step() both the optimizer and the scheduler!\n",
    "        ### CODE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad374b",
   "metadata": {
    "id": "wqq7Am1x5e_0",
    "papermill": {
     "duration": 0.017974,
     "end_time": "2024-05-04T08:17:31.870067",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.852093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020ab725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:31.904918Z",
     "iopub.status.busy": "2024-05-04T08:17:31.904418Z",
     "iopub.status.idle": "2024-05-04T08:17:32.475264Z",
     "shell.execute_reply": "2024-05-04T08:17:32.474517Z"
    },
    "id": "l6N_UFgF5e_0",
    "papermill": {
     "duration": 0.590626,
     "end_time": "2024-05-04T08:17:32.477536",
     "exception": false,
     "start_time": "2024-05-04T08:17:31.886910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a40e6",
   "metadata": {
    "id": "mUzpRkRV5e_0",
    "papermill": {
     "duration": 0.016286,
     "end_time": "2024-05-04T08:17:32.510923",
     "exception": false,
     "start_time": "2024-05-04T08:17:32.494637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60fff0",
   "metadata": {
    "id": "fGBe-hGS5e_1",
    "papermill": {
     "duration": 0.016162,
     "end_time": "2024-05-04T08:17:32.543857",
     "exception": false,
     "start_time": "2024-05-04T08:17:32.527695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs.\n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00abcd9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:32.577940Z",
     "iopub.status.busy": "2024-05-04T08:17:32.577644Z",
     "iopub.status.idle": "2024-05-04T08:17:32.932400Z",
     "shell.execute_reply": "2024-05-04T08:17:32.931682Z"
    },
    "id": "k9FfBL5j5e_1",
    "papermill": {
     "duration": 0.374286,
     "end_time": "2024-05-04T08:17:32.934501",
     "exception": false,
     "start_time": "2024-05-04T08:17:32.560215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19731d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:32.969786Z",
     "iopub.status.busy": "2024-05-04T08:17:32.969480Z",
     "iopub.status.idle": "2024-05-04T08:17:33.018339Z",
     "shell.execute_reply": "2024-05-04T08:17:33.017296Z"
    },
    "id": "iboaQjph5e_1",
    "outputId": "69c68f39-08a7-47e1-e46a-64ca10e67b95",
    "papermill": {
     "duration": 0.068369,
     "end_time": "2024-05-04T08:17:33.020308",
     "exception": false,
     "start_time": "2024-05-04T08:17:32.951939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1deff7",
   "metadata": {
    "id": "4S0D_LO25e_2",
    "papermill": {
     "duration": 0.016601,
     "end_time": "2024-05-04T08:17:33.053790",
     "exception": false,
     "start_time": "2024-05-04T08:17:33.037189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b352acf",
   "metadata": {
    "id": "X7YZKIxn5e_2",
    "papermill": {
     "duration": 0.016446,
     "end_time": "2024-05-04T08:17:33.086962",
     "exception": false,
     "start_time": "2024-05-04T08:17:33.070516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eede2560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:33.121898Z",
     "iopub.status.busy": "2024-05-04T08:17:33.121502Z",
     "iopub.status.idle": "2024-05-04T08:17:35.541632Z",
     "shell.execute_reply": "2024-05-04T08:17:35.540844Z"
    },
    "id": "L5-GlahX5e_2",
    "outputId": "c51fb30d-44bd-42c0-bf5a-c50d56301f82",
    "papermill": {
     "duration": 2.439976,
     "end_time": "2024-05-04T08:17:35.543771",
     "exception": false,
     "start_time": "2024-05-04T08:17:33.103795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132176f1",
   "metadata": {
    "id": "fl99blVf5e_2",
    "papermill": {
     "duration": 0.016624,
     "end_time": "2024-05-04T08:17:35.577711",
     "exception": false,
     "start_time": "2024-05-04T08:17:35.561087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe0f85",
   "metadata": {
    "id": "Y0_GK3V75e_2",
    "papermill": {
     "duration": 0.05593,
     "end_time": "2024-05-04T08:17:35.650344",
     "exception": false,
     "start_time": "2024-05-04T08:17:35.594414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484fc9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T08:17:35.686835Z",
     "iopub.status.busy": "2024-05-04T08:17:35.686323Z"
    },
    "id": "iScNN_-85e_2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-05-04T08:17:35.667742",
     "status": "running"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action + 1)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory\n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards')\n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"breakout_dqn.png\") # save graph for training visualization\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                # print(\"hi\")\n",
    "                torch.save(agent.policy_net, \"breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7adc12",
   "metadata": {
    "id": "Z5ev0UH45e_3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf2eb4",
   "metadata": {
    "id": "fuzUdssS5e_3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630c67b",
   "metadata": {
    "id": "dzF8x3JD5e_3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
    "state = env.reset()\n",
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env\n",
    "\n",
    "from agent import Agent\n",
    "action_size = 3\n",
    "\n",
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/breakout_dqn_double.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state, _ = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "frame = 0\n",
    "while not done:\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "\n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "\n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1)\n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory\n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "\n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-04T08:15:50.173013",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
