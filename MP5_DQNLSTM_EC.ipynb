{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gym==0.26.2 gym-notices==0.0.8\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg\n",
        "!pip3 install gym pyvirtualdisplay\n",
        "!pip install xvfbwrapper pyvirtualdisplay PyOpenGL ffmpeg-python"
      ],
      "metadata": {
        "id": "bkgy2SbPl1Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJjS2M6VkjV"
      },
      "source": [
        "# Creating a DQN LSTM Agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade setuptools --user\n",
        "!pip3 install ez_setup\n",
        "!pip3 install gym[atari]\n",
        "!pip3 install gym[accept-rom-license]"
      ],
      "metadata": {
        "id": "-UJHlD1XdMVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 3500\n",
        "HEIGHT = 84\n",
        "WIDTH = 84\n",
        "HISTORY_SIZE = 4\n",
        "learning_rate = 0.0001\n",
        "evaluation_reward_length = 100\n",
        "Memory_capacity = 1000000\n",
        "train_frame = 100000 # You can set it to a lower value while testing your code so you don't have to wait longer to see if the training code does not have any syntax errors\n",
        "batch_size = 128\n",
        "scheduler_gamma = 0.4\n",
        "scheduler_step_size = 100000\n",
        "\n",
        "# Hyperparameters for Double DQN agent\n",
        "update_target_network_frequency = 1000\n",
        "\n",
        "# Hyperparameters for DQN LSTM agent\n",
        "lstm_seq_length = 5"
      ],
      "metadata": {
        "id": "Q_8tLb617XSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QnhHG_NVkjY",
        "outputId": "69bfa180-04a5-4168-ca2a-399c247aaeb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_max_lives(env):\n",
        "    env.reset()\n",
        "    _, _, _, _, info = env.step(0)\n",
        "    return info['lives']\n",
        "\n",
        "def check_live(life, cur_life):\n",
        "    if life > cur_life:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_frame(X):\n",
        "    x = np.uint8(resize(rgb2gray(X), (HEIGHT, WIDTH), mode='reflect') * 255)\n",
        "    return x\n",
        "\n",
        "def get_init_state(history, s, history_size):\n",
        "    for i in range(history_size):\n",
        "        history[i, :, :] = get_frame(s)"
      ],
      "metadata": {
        "id": "IFilZvJ-7YKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent_LSTM(Agent):\n",
        "    def __init__(self, action_size):\n",
        "        super().__init__(action_size)\n",
        "        self.memory = ReplayMemoryLSTM()\n",
        "        self.policy_net = DQN_LSTM(action_size)\n",
        "        self.policy_net.to(device)\n",
        "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "\n",
        "    def get_action(self, state, hidden=None):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = torch.tensor(np.random.randint(self.action_size)).to(device)\n",
        "            _, hidden = self.policy_net(state.unsqueeze(0), hidden, train=False)\n",
        "        else:\n",
        "            q_values, hidden = self.policy_net(state.unsqueeze(0), hidden, train=False)\n",
        "            action = q_values.max(1)[1]\n",
        "        return action, hidden\n",
        "\n",
        "    def train_policy_net(self, frame):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "\n",
        "        mini_batch = self.memory.sample_mini_batch(frame)\n",
        "        mini_batch = np.array(mini_batch, dtype=object).transpose()\n",
        "\n",
        "        history = np.stack(mini_batch[0], axis=0)\n",
        "        states = np.float32(history[:, :lstm_seq_length, :, :]) / 255.\n",
        "        states = torch.from_numpy(states).to(device)\n",
        "        actions = list(mini_batch[1])\n",
        "        actions = torch.LongTensor(actions).to(device)\n",
        "        rewards = list(mini_batch[2])\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
        "        next_states = torch.from_numpy(next_states).to(device)\n",
        "        dones = mini_batch[3]\n",
        "        mask = torch.tensor(list(map(int, dones == False)), dtype=torch.uint8).to(device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values, _ = self.policy_net(states, None)\n",
        "        state_action_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values, _ = self.policy_net(next_states, None)\n",
        "        next_state_values = next_q_values.max(1)[0]\n",
        "        next_state_values = next_state_values[mask]\n",
        "        expected_state_action_values = (next_state_values.detach() * self.discount_factor) + rewards[mask]\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(state_action_values, expected_state_action_values)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()"
      ],
      "metadata": {
        "id": "QPUd9bvn7tGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque(maxlen=Memory_capacity)\n",
        "\n",
        "    def push(self, history, action, reward, done):\n",
        "        self.memory.append((history, action, reward, done))\n",
        "\n",
        "    def sample_mini_batch(self, frame):\n",
        "        mini_batch = []\n",
        "        if frame >= Memory_capacity:\n",
        "            sample_range = Memory_capacity\n",
        "        else:\n",
        "            sample_range = frame\n",
        "\n",
        "        # history size\n",
        "        sample_range -= (HISTORY_SIZE + 1)\n",
        "\n",
        "        idx_sample = random.sample(range(sample_range), batch_size)\n",
        "        for i in idx_sample:\n",
        "            sample = []\n",
        "            for j in range(HISTORY_SIZE + 1):\n",
        "                sample.append(self.memory[i + j])\n",
        "\n",
        "            sample = np.array(sample, dtype=object)\n",
        "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[3, 1], sample[3, 2], sample[3, 3]))\n",
        "\n",
        "        return mini_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class ReplayMemoryLSTM(ReplayMemory):\n",
        "    \"\"\"\n",
        "    This is a version of Replay Memory modified for LSTMs.\n",
        "    Replay memory generally stores (state, action, reward, next state).\n",
        "    But LSTMs need sequential data.\n",
        "    So we store (state, action, reward, next state) for previous few states, constituting a trajectory.\n",
        "    During training, the previous states will be used to generate the current state of LSTM.\n",
        "    Note that samples from previous episode might get included in the trajectory.\n",
        "    Inspite of not being fully correct, this simple Replay Buffer performs well.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def sample_mini_batch(self, frame):\n",
        "        mini_batch = []\n",
        "        if frame >= Memory_capacity:\n",
        "            sample_range = Memory_capacity\n",
        "        else:\n",
        "            sample_range = frame\n",
        "\n",
        "        sample_range -= (lstm_seq_length + 1)\n",
        "\n",
        "        idx_sample = random.sample(range(sample_range - lstm_seq_length), batch_size)\n",
        "        for i in idx_sample:\n",
        "            sample = []\n",
        "            for j in range(lstm_seq_length + 1):\n",
        "                sample.append(self.memory[i + j])\n",
        "\n",
        "            sample = np.array(sample, dtype=object)\n",
        "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[lstm_seq_length - 1, 1], sample[lstm_seq_length - 1, 2], sample[lstm_seq_length - 1, 3]))\n",
        "\n",
        "        return mini_batch\n"
      ],
      "metadata": {
        "id": "44RcQ2Nz7P2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxXBWUIoVkjZ"
      },
      "outputs": [],
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwi-Lqz7VkjZ",
        "outputId": "1f159ea4-39ac-450c-e6cd-88e9d6c02881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB87NRWTVkjZ"
      },
      "source": [
        "Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next state) tuples rather than just one (state, action, reward, next state) tuple so it can work with LSTMs. Use the previous tuples to generate the current hidden and context vector for LSTM.\n",
        "Esentially, when you get a sample from replay buffer during training, start with the first tuple and generate hidden and context vector from this and pass it onto the next tuple. Do so consequitively till you reach the last tuple, where you will make Q value predictions.\n",
        "Training loop remains nearly the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z_GsZcoVkja"
      },
      "outputs": [],
      "source": [
        "agent = Agent_LSTM(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn6gtHoqVkja"
      },
      "outputs": [],
      "source": [
        "HISTORY_SIZE = 1\n",
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "    hidden = None\n",
        "\n",
        "    get_init_state(history, state, HISTORY_SIZE)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "\n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[1, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['lives'])\n",
        "\n",
        "        life = info['lives']\n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "        score += reward\n",
        "        history[:1, :, :] = history[1:, :, :]\n",
        "\n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards')\n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n",
        "\n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn_lstm.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbE59U3ZVkja"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaX_x2IxVkjb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN, DQN_LSTM\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
        "state = env.reset()\n",
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = RecordVideo(env, './video')\n",
        "    return env\n",
        "\n",
        "from agent import Agent\n",
        "action_size = 3\n",
        "\n",
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "agent = Agent(action_size)\n",
        "agent.load_policy_net(\"./save_model/breakout_dqn_lstm.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state, _ = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state, HISTORY_SIZE)\n",
        "frame = 0\n",
        "while not done:\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "    state = next_state\n",
        "\n",
        "    next_state, reward, done, _, info = env.step(action + 1)\n",
        "\n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['lives'])\n",
        "\n",
        "    life = info['lives']\n",
        "    r = np.clip(reward, -1, 1)\n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory\n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "\n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}